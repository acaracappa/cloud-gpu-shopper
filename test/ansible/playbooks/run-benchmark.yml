---
# Run benchmark suite against deployed vLLM
#
# Usage:
#   ansible-playbook playbooks/run-benchmark.yml -i inventory/dynamic.py \
#     -e results_dir=/tmp/benchmark-results \
#     -e benchmark_client=/usr/local/bin/benchmark-client

- name: Run benchmark suite
  hosts: benchmark_nodes
  become: yes
  gather_facts: yes

  vars:
    # Benchmark configuration
    vllm_endpoint: "http://localhost:{{ vllm_port | default(8000) }}"
    results_dir: "{{ results_path | default('/tmp/benchmark-results') }}"
    benchmark_client: "{{ client_path | default('/usr/local/bin/benchmark-client') }}"

    # Benchmark parameters
    throughput_requests: "{{ tp_requests | default(100) }}"
    latency_requests: "{{ lat_requests | default(50) }}"
    concurrency_max: "{{ max_concurrent | default(32) }}"

    # Timing
    benchmark_start_time: "{{ ansible_date_time.epoch }}"

  pre_tasks:
    - name: Create results directory
      ansible.builtin.file:
        path: "{{ results_dir }}"
        state: directory
        mode: '0755'

    - name: Verify vLLM is running
      ansible.builtin.uri:
        url: "{{ vllm_endpoint }}/health"
        method: GET
        status_code: 200
      register: vllm_health
      retries: 3
      delay: 5
      until: vllm_health.status == 200

    - name: Get model info
      ansible.builtin.uri:
        url: "{{ vllm_endpoint }}/v1/models"
        method: GET
        return_content: yes
      register: model_info

    - name: Set model ID fact
      ansible.builtin.set_fact:
        current_model_id: "{{ model_info.json.data[0].id }}"

    - name: Display benchmark configuration
      ansible.builtin.debug:
        msg: |
          Benchmark Configuration:
            Endpoint: {{ vllm_endpoint }}
            Model: {{ current_model_id }}
            Results: {{ results_dir }}
            Throughput Requests: {{ throughput_requests }}
            Latency Requests: {{ latency_requests }}
            Max Concurrency: {{ concurrency_max }}

  tasks:
    - name: Run throughput benchmark
      ansible.builtin.command: >
        {{ benchmark_client }} throughput
        --endpoint {{ vllm_endpoint }}
        --requests {{ throughput_requests }}
        --output {{ results_dir }}/throughput.json
      register: throughput_result
      retries: 3
      delay: 10
      until: throughput_result.rc == 0
      ignore_errors: yes

    - name: Display throughput result
      ansible.builtin.debug:
        msg: "Throughput benchmark: {{ 'PASSED' if throughput_result.rc == 0 else 'FAILED' }}"

    - name: Run latency benchmark
      ansible.builtin.command: >
        {{ benchmark_client }} latency
        --endpoint {{ vllm_endpoint }}
        --requests {{ latency_requests }}
        --output {{ results_dir }}/latency.json
      register: latency_result
      retries: 3
      delay: 10
      until: latency_result.rc == 0
      ignore_errors: yes

    - name: Display latency result
      ansible.builtin.debug:
        msg: "Latency benchmark: {{ 'PASSED' if latency_result.rc == 0 else 'FAILED' }}"

    - name: Run concurrency benchmark
      ansible.builtin.command: >
        {{ benchmark_client }} concurrency
        --endpoint {{ vllm_endpoint }}
        --max-concurrent {{ concurrency_max }}
        --output {{ results_dir }}/concurrency.json
      register: concurrency_result
      retries: 3
      delay: 10
      until: concurrency_result.rc == 0
      ignore_errors: yes

    - name: Display concurrency result
      ansible.builtin.debug:
        msg: "Concurrency benchmark: {{ 'PASSED' if concurrency_result.rc == 0 else 'FAILED' }}"

    - name: Collect failed steps
      ansible.builtin.set_fact:
        failed_steps: >-
          {{
            ([] if throughput_result.rc == 0 else ['throughput']) +
            ([] if latency_result.rc == 0 else ['latency']) +
            ([] if concurrency_result.rc == 0 else ['concurrency'])
          }}

    - name: Create summary file
      ansible.builtin.copy:
        content: |
          {
            "model_id": "{{ current_model_id }}",
            "endpoint": "{{ vllm_endpoint }}",
            "host": "{{ inventory_hostname }}",
            "timestamp": "{{ ansible_date_time.iso8601 }}",
            "results": {
              "throughput": {{ 'true' if throughput_result.rc == 0 else 'false' }},
              "latency": {{ 'true' if latency_result.rc == 0 else 'false' }},
              "concurrency": {{ 'true' if concurrency_result.rc == 0 else 'false' }}
            },
            "failed_steps": {{ failed_steps | to_json }}
          }
        dest: "{{ results_dir }}/summary.json"
        mode: '0644'

  post_tasks:
    - name: Fetch throughput results
      ansible.builtin.fetch:
        src: "{{ results_dir }}/throughput.json"
        dest: "./results/{{ inventory_hostname }}/"
        flat: yes
      when: throughput_result.rc == 0
      ignore_errors: yes

    - name: Fetch latency results
      ansible.builtin.fetch:
        src: "{{ results_dir }}/latency.json"
        dest: "./results/{{ inventory_hostname }}/"
        flat: yes
      when: latency_result.rc == 0
      ignore_errors: yes

    - name: Fetch concurrency results
      ansible.builtin.fetch:
        src: "{{ results_dir }}/concurrency.json"
        dest: "./results/{{ inventory_hostname }}/"
        flat: yes
      when: concurrency_result.rc == 0
      ignore_errors: yes

    - name: Fetch summary
      ansible.builtin.fetch:
        src: "{{ results_dir }}/summary.json"
        dest: "./results/{{ inventory_hostname }}/"
        flat: yes

    - name: Calculate benchmark duration
      ansible.builtin.set_fact:
        benchmark_duration: "{{ (ansible_date_time.epoch | int) - (benchmark_start_time | int) }}"

    - name: Display benchmark summary
      ansible.builtin.debug:
        msg: |
          Benchmark Complete!
          Duration: {{ benchmark_duration }} seconds
          Model: {{ current_model_id }}

          Results:
            Throughput: {{ 'PASSED' if throughput_result.rc == 0 else 'FAILED' }}
            Latency: {{ 'PASSED' if latency_result.rc == 0 else 'FAILED' }}
            Concurrency: {{ 'PASSED' if concurrency_result.rc == 0 else 'FAILED' }}

          {% if failed_steps | length > 0 %}
          Failed steps: {{ failed_steps | join(', ') }}
          {% else %}
          All benchmarks passed!
          {% endif %}

          Results saved to: ./results/{{ inventory_hostname }}/

    - name: Fail if all benchmarks failed
      ansible.builtin.fail:
        msg: "All benchmarks failed. Check logs for details."
      when: failed_steps | length == 3
