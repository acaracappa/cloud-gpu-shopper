---
# Deploy vLLM with a specific model
#
# Usage:
#   ansible-playbook playbooks/deploy-vllm.yml -i inventory/dynamic.py \
#     -e model_id=mistralai/Mistral-7B-Instruct-v0.3 \
#     -e tensor_parallel=1 \
#     -e max_model_len=4096

- name: Deploy vLLM with model
  hosts: benchmark_nodes
  become: yes
  gather_facts: yes

  vars:
    # Model configuration (required)
    model_id: "{{ model | default('') }}"

    # Optional configuration with sensible defaults
    tensor_parallel: "{{ tp | default(1) }}"
    max_model_len: "{{ max_len | default(4096) }}"
    gpu_memory_utilization: "{{ gpu_util | default(0.90) }}"
    vllm_port: "{{ port | default(8000) }}"

    # Timing
    deploy_start_time: "{{ ansible_date_time.epoch }}"

  pre_tasks:
    - name: Validate model_id is provided
      ansible.builtin.fail:
        msg: "model_id is required. Use -e model_id=<huggingface_model_id>"
      when: model_id | length == 0

    - name: Display deployment configuration
      ansible.builtin.debug:
        msg: |
          Deploying vLLM:
            Model: {{ model_id }}
            Tensor Parallel: {{ tensor_parallel }}
            Max Model Length: {{ max_model_len }}
            GPU Memory Utilization: {{ gpu_memory_utilization }}
            Port: {{ vllm_port }}

    - name: Check current GPU memory
      ansible.builtin.shell: nvidia-smi --query-gpu=memory.free,memory.total --format=csv,noheader
      register: gpu_memory
      changed_when: false

    - name: Display available GPU memory
      ansible.builtin.debug:
        msg: "GPU Memory (free/total): {{ gpu_memory.stdout }}"

  roles:
    - role: vllm
      vars:
        vllm_model_id: "{{ model_id }}"
        vllm_tensor_parallel: "{{ tensor_parallel }}"
        vllm_max_model_len: "{{ max_model_len }}"
        vllm_gpu_memory_utilization: "{{ gpu_memory_utilization }}"
        vllm_port: "{{ vllm_port }}"
      tags: [vllm]

  post_tasks:
    - name: Test inference endpoint
      ansible.builtin.uri:
        url: "http://localhost:{{ vllm_port }}/v1/completions"
        method: POST
        body_format: json
        body:
          model: "{{ model_id }}"
          prompt: "Hello, "
          max_tokens: 10
        headers:
          Content-Type: application/json
        status_code: 200
        return_content: yes
      register: test_inference
      retries: 3
      delay: 5
      until: test_inference.status == 200

    - name: Display test inference result
      ansible.builtin.debug:
        msg: "Test inference successful: {{ test_inference.json.choices[0].text | default('OK') | truncate(50) }}"

    - name: Calculate deployment duration
      ansible.builtin.set_fact:
        deploy_duration: "{{ (ansible_date_time.epoch | int) - (deploy_start_time | int) }}"

    - name: Display deployment summary
      ansible.builtin.debug:
        msg: |
          vLLM Deployment Complete!
          Duration: {{ deploy_duration }} seconds
          Model: {{ model_id }}
          Endpoint: http://{{ inventory_hostname }}:{{ vllm_port }}
          Health: http://{{ inventory_hostname }}:{{ vllm_port }}/health
          Ready for benchmarking!
